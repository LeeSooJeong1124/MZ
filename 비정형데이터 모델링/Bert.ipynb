{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM8SwOFjW/VYYmsRu/tnmJe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["https://github.com/kyawkhaung/researcharticles/blob/main/classification/2%20Abstract%20Only.ipynb"],"metadata":{"id":"d183OULcwV9L"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFYodpvwwFX2"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","\n","import transformers\n","import torch\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer, BertModel, BertConfig"]},{"cell_type":"code","source":["data_path = \"/content/drive/Shareddrives/22-2 캡스톤 프로젝트/데이터/preprocessed_essay_for_bert.csv\"\n","df_raw = pd.read_csv(data_path, encoding='utf-8-sig')\n","\n","df_raw = df_raw.replace('y', 1)\n","df_raw = df_raw.replace('n', 0)\n","\n","print(df_raw.shape)\n","df_raw"],"metadata":{"id":"cLW75wuBwc03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_raw['target_list'] = df_raw[['cEXT', 'cNEU', 'cCON']].values.tolist()\n","testlist = df_raw[['cEXT', 'cNEU', 'cCON']].values"],"metadata":{"id":"gFS0hYeNwu_c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["testlist"],"metadata":{"id":"IgsP69gKwvUF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_raw['WORD_COUNT'] = df_raw['TEXT_KOR'].apply(lambda x: len(x.split()))\n","df_raw"],"metadata":{"id":"Cw0LWCFdwvon"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('평균:', sum(df_raw['WORD_COUNT']) / len(df_raw))\n","print('최대:', max(df_raw['WORD_COUNT']))\n","df_raw.hist('WORD_COUNT', bins=30)"],"metadata":{"id":"IHWJFqn_xLHJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2 = df_raw[['TEXT_KOR', 'target_list']].copy()\n","df2"],"metadata":{"id":"Te3xoSHHxLJa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_LEN = 128\n","TRAIN_BATCH_SIZE = 320\n","VALID_BATCH_SIZE = 320\n","EPOCHS = 5\n","LEARNING_RATE = 1e-05\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"],"metadata":{"id":"lJ-pChpXxLL5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.title = dataframe['TEXT_KOR']\n","        self.targets = self.data.target_list\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.title)\n","\n","    def __getitem__(self, index):\n","        title = str(self.title[index])\n","        title = \" \".join(title.split())\n","\n","        inputs = self.tokenizer.encode_plus(\n","            title,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            return_token_type_ids=True,\n","            truncation=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n","        }"],"metadata":{"id":"w8tLmcIuxPUy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_size = 0.8\n","train_dataset = df2.sample(frac=train_size, random_state=42)\n","valid_dataset = df2.drop(train_dataset.index).reset_index(drop=True)\n","train_dataset = train_dataset.reset_index(drop=True)\n","\n","print(\"FULL Dataset: {}\".format(df2.shape))\n","print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","print(\"TEST Dataset: {}\".format(valid_dataset.shape))\n","\n","training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n","validation_set = CustomDataset(valid_dataset, tokenizer, MAX_LEN)"],"metadata":{"id":"lRyWR8OaxPXZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_params = {'batch_size': TRAIN_BATCH_SIZE,\n","                'shuffle': True,\n","                'num_workers': 0\n","                }\n","\n","test_params = {'batch_size': VALID_BATCH_SIZE,\n","                'shuffle': False,\n","                'num_workers': 0\n","                }\n","\n","training_loader = DataLoader(training_set, **train_params)\n","validation_loader = DataLoader(validation_set, **test_params)"],"metadata":{"id":"d-XUoaXbxPZ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(training_loader)"],"metadata":{"id":"vPugF-mFxPcO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating the customized model, by adding a drop out and a dense layer on top of distil\n","# bert to get the final output for the model. \n","\n","def load_ckp(checkpoint_fpath, model, optimizer):\n","    \"\"\"\n","    checkpoint_path: path to save checkpoint\n","    model: model that we want to load checkpoint parameters into       \n","    optimizer: optimizer we defined in previous training\n","    \"\"\"\n","    # load check point\n","    checkpoint = torch.load(checkpoint_fpath)\n","    # initialize state_dict from checkpoint to model\n","    model.load_state_dict(checkpoint['state_dict'])\n","    # initialize optimizer from checkpoint to optimizer\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    # initialize valid_loss_min from checkpoint to valid_loss_min\n","    valid_loss_min = checkpoint['valid_loss_min']\n","    # return model, optimizer, epoch value, min validation loss \n","    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()"],"metadata":{"id":"PD1PkQs-xPet"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BERTClass(torch.nn.Module):\n","    def __init__(self):\n","        super(BERTClass, self).__init__()\n","        self.l1 = transformers.BertModel.from_pretrained('bert-base-multilingual-cased')\n","        self.l2 = torch.nn.Dropout(0.3)\n","        self.l3 = torch.nn.Linear(768, 3)\n","    \n","    def forward(self, ids, mask, token_type_ids):\n","        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n","        output_2 = self.l2(output_1)\n","        output = self.l3(output_2)\n","        return output"],"metadata":{"id":"j-lKbCM0xPhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BERTClass()\n","\n","device = torch.device('cpu')\n","model.to(device)"],"metadata":{"id":"San5lCQRxaST"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def loss_fn(outputs, targets):\n","    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n","\n","optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)"],"metadata":{"id":"IskRCpkbxaU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_ckp(checkpoint_fpath, model, optimizer):\n","    \"\"\"\n","    checkpoint_path: path to save checkpoint\n","    model: model that we want to load checkpoint parameters into       \n","    optimizer: optimizer we defined in previous training\n","    \"\"\"\n","    # load check point\n","    checkpoint = torch.load(checkpoint_fpath)\n","    # initialize state_dict from checkpoint to model\n","    model.load_state_dict(checkpoint['state_dict'])\n","    # initialize optimizer from checkpoint to optimizer\n","    optimizer.load_state_dict(checkpoint['optimizer'])\n","    # initialize valid_loss_min from checkpoint to valid_loss_min\n","    valid_loss_min = checkpoint['valid_loss_min']\n","    # return model, optimizer, epoch value, min validation loss \n","    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()"],"metadata":{"id":"8XeXQMmxxaXk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shutil, sys   \n","def save_ckp(state, is_best, checkpoint_path, best_model_path):\n","    \"\"\"\n","    state: checkpoint we want to save\n","    is_best: is this the best checkpoint; min validation loss\n","    checkpoint_path: path to save checkpoint\n","    best_model_path: path to save best model\n","    \"\"\"\n","    f_path = checkpoint_path\n","    # save checkpoint data to the path given, checkpoint_path\n","    torch.save(state, f_path)\n","    # if it is a best model, min validation loss\n","    if is_best:\n","        best_fpath = best_model_path\n","        # copy that checkpoint file to best path given, best_model_path\n","        shutil.copyfile(f_path, best_fpath)"],"metadata":{"id":"_09qIqGBxequ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#to use as global variables\n","val_targets=[]\n","val_outputs=[]"],"metadata":{"id":"djihyeBExetD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(start_epochs, n_epochs, valid_loss_min_input, \n","                training_loader, validation_loader, model, \n","                optimizer, checkpoint_path, best_model_path):\n","   \n","    # initialize tracker for minimum validation loss\n","    valid_loss_min = valid_loss_min_input \n","    \n","    for epoch in range(start_epochs, n_epochs+1):\n","        train_loss = 0\n","        valid_loss = 0\n","        \n","        model.train()\n","        print('############# Epoch {}: Training Start   #############'.format(epoch))\n","        \n","        for batch_idx, data in enumerate(training_loader):\n","            #print('yyy epoch', batch_idx)\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            targets = data['targets'].to(device, dtype = torch.float)\n","\n","            outputs = model(ids, mask, token_type_ids)\n","\n","            optimizer.zero_grad()\n","            loss = loss_fn(outputs, targets)\n","            # if batch_idx%5000==0:\n","            #    print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n","        \n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            #print('before loss data in training', loss.item(), train_loss)\n","            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n","            #print('after loss data in training', loss.item(), train_loss)\n","    \n","        print('############# Epoch {}: Training End     #############'.format(epoch))\n","    \n","        print('############# Epoch {}: Validation Start   #############'.format(epoch))\n","        ######################    \n","        # validate the model #\n","        ######################\n"," \n","        model.eval()\n","   \n","        with torch.no_grad():\n","            for batch_idx, data in enumerate(validation_loader, 0):\n","                ids = data['ids'].to(device, dtype = torch.long)\n","                mask = data['mask'].to(device, dtype = torch.long)\n","                token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","                targets = data['targets'].to(device, dtype = torch.float)\n","                outputs = model(ids, mask, token_type_ids)\n","\n","                loss = loss_fn(outputs, targets)\n","                valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n","                val_targets.extend(targets.cpu().detach().numpy().tolist())\n","                val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n","        \n","            print('############# Epoch {}: Validation End     #############'.format(epoch))\n","            # calculate average losses\n","            # print('before cal avg train loss', train_loss)\n","            train_loss = train_loss/len(training_loader)\n","            valid_loss = valid_loss/len(validation_loader)\n","            # print training/validation statistics \n","            print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(epoch,train_loss,valid_loss))\n","      \n","            # create checkpoint variable and add important data\n","            checkpoint = {\n","              'epoch': epoch + 1,\n","              'valid_loss_min': valid_loss,\n","              'state_dict': model.state_dict(),\n","              'optimizer': optimizer.state_dict()\n","            }\n","        \n","            # save checkpoint\n","            save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n","        \n","            ## TODO: save the model if validation loss has decreased\n","            if valid_loss <= valid_loss_min:\n","                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n","                # save checkpoint as best model\n","                save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n","                valid_loss_min = valid_loss\n","\n","            print('############# Epoch {}  Done   #############\\n'.format(epoch))\n","    \n","    return model"],"metadata":{"id":"ihcMxXkPxe2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint_path = 'current_checkpoint.pt'\n","best_model = 'bert_best_model.pt'\n","trained_model = train_model(1, 5, np.Inf, training_loader, validation_loader, model, optimizer, checkpoint_path, best_model)"],"metadata":{"id":"2cngDgVNxLOS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_preds = (np.array(val_outputs) > 0.5).astype(int)\n","val_preds"],"metadata":{"id":"4e_qMYNxxlwX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy = metrics.accuracy_score(val_targets, val_preds)\n","f1_score_micro = metrics.f1_score(val_targets, val_preds, average='micro')\n","f1_score_macro = metrics.f1_score(val_targets, val_preds, average='macro')\n","print(f\"Accuracy Score = {accuracy}\")\n","print(f\"F1 Score (Micro) = {f1_score_micro}\")\n","print(f\"F1 Score (Macro) = {f1_score_macro}\")"],"metadata":{"id":"W_0_HrJIxLQm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cxcC_1yixoOe"},"execution_count":null,"outputs":[]}]}